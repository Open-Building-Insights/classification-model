{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial configuration\n",
    "### To start working with this particular notebook, you need to provide necessary credential and settings\n",
    "### Below is an template of configuration, which is necessary prepare aside of this notebook and copy & paste all content in triple quotes to the next cell's input field\n",
    "    \"\"\"\n",
    "    {\n",
    "    \"ML_MODELS_BUCKET\": \"ml-saved-models\",\n",
    "    \"ML_MODELS_BUCKET_CRN\": \"xxx\",\n",
    "    \"CLOUDANT_API_KEY\": \"xxx\",\n",
    "    \"CLOUDANT_URL\": \"xxx\",\n",
    "    \"CLOUDANT_UI_DB_API_KEY\": \"xxx\",\n",
    "    \"CLOUDANT_UI_DB_URL\": \"xxx\",\n",
    "    \"UTILS_BUCKET\": \"notebook-utils-bucket\",\n",
    "    \"BUCKET_TIFF\": \"kenya-images\",\n",
    "    \"HEIGHTS_TIFF_FILENAME\": \"WSF3Dv3_Kenya.tif\",\n",
    "    \"DB_NAME\": \"features_db\",\n",
    "    \"UI_DB_NAME\": \"buildings_db\",\n",
    "    \"COS_ENDPOINT_URL\": \"xxx\",\n",
    "    \"COS_APIKEY\": \"xxx\",\n",
    "    \"TYPE_SOURCE_FILTER\": \"area\",\n",
    "    \"AREA_TRESHOLD\": 0\n",
    "    }\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import json\n",
    "\n",
    "config_str = getpass.getpass('Enter your prepared config: ')\n",
    "config = json.loads(config_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install ibmcloudant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import shutil\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn.metrics as SKM\n",
    "import ibm_boto3\n",
    "from botocore.client import Config\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "\n",
    "from ibmcloudant.cloudant_v1 import CloudantV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "import cv2\n",
    "\n",
    "import ibmcloudant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init cloudant db client\n",
    "authenticator = IAMAuthenticator(config[\"CLOUDANT_API_KEY\"])\n",
    "client = CloudantV1(authenticator=authenticator)\n",
    "client.set_service_url(config[\"CLOUDANT_URL\"])\n",
    "\n",
    "# init COS client\n",
    "data_conn = ibm_boto3.resource(service_name='s3',\n",
    "    ibm_api_key_id=config[\"COS_APIKEY\"],\n",
    "    ibm_service_instance_id=config[\"ML_MODELS_BUCKET_CRN\"],\n",
    "    ibm_auth_endpoint='https://iam.bluemix.net/oidc/token',\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=config[\"COS_ENDPOINT_URL\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download & prepare testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init data path for dataset\n"
     ]
    }
   ],
   "source": [
    "print('Init data path for dataset')\n",
    "base_path = os.getcwd()\n",
    "data_path = os.path.join(base_path, 'Sentinel2_set/')\n",
    "\n",
    "# createmtrain, test and validation folders\n",
    "train_path = os.path.join(data_path, 'train/')\n",
    "test_path = os.path.join(data_path, 'test/')\n",
    "validation_path = os.path.join(data_path, 'validation/')\n",
    "\n",
    "# create residential and nonresidential subfolders\n",
    "train_res_path = os.path.join(train_path, 'residential')\n",
    "train_nonres_path = os.path.join(train_path, 'nonresidential')\n",
    "\n",
    "test_res_path = os.path.join(test_path, 'residential')\n",
    "test_nonres_path = os.path.join(test_path, 'nonresidential')\n",
    "\n",
    "validation_res_path = os.path.join(validation_path, 'residential')\n",
    "validation_nonres_path = os.path.join(validation_path, 'nonresidential')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete local res and nonres directories, if exists (in ordet to have updated data)\n"
     ]
    }
   ],
   "source": [
    "res_path = os.path.join(data_path, 'residential')\n",
    "nonres_path = os.path.join(data_path, 'nonresidential')\n",
    "\n",
    "print('Delete local res and nonres directories, if exists (in ordet to have updated data)')\n",
    "try:\n",
    "    shutil.rmtree(train_res_path)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    shutil.rmtree(test_res_path)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    shutil.rmtree(validation_res_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(train_nonres_path)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    shutil.rmtree(test_nonres_path)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    shutil.rmtree(validation_nonres_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.makedirs(train_res_path, exist_ok = True)\n",
    "os.makedirs(test_res_path, exist_ok = True)\n",
    "os.makedirs(validation_res_path, exist_ok = True)\n",
    "\n",
    "os.makedirs(train_nonres_path, exist_ok = True)\n",
    "os.makedirs(test_nonres_path, exist_ok = True)\n",
    "os.makedirs(validation_nonres_path, exist_ok = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images revealed in the database: 6155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Images processed: 100%|██████████| 6155/6155 [01:22<00:00, 74.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# fetch OSM testing data\n",
    "response = client.post_find(\n",
    "    db=config[\"DB_NAME\"],\n",
    "    selector={\n",
    "        \"properties.type_source\": \"osm\", # filter for OSM entries\n",
    "        \"properties.image_ML_type\": \"test\", # filter for OSM entries\n",
    "        \"_attachments\": {\"$exists\": True}, # filter only exists attachments\n",
    "        \"properties.area_in_meters\": { \"$gt\": 20}\n",
    "        },\n",
    "    fields=[\"_id\", \"_attachments\", \"properties.image_ML_type\", \"properties.image_ML_class\"],\n",
    ").get_result()\n",
    "\n",
    "print(f\"Images revealed in the database: {len(response['docs'])}\")\n",
    "# store id and data for all attachments\n",
    "\n",
    "for document in tqdm(response[\"docs\"], desc='Images processed'):\n",
    "    try: \n",
    "        image_ML_type = document['properties']['image_ML_type'] # use this to assign to appropriate folder train, test, validation\n",
    "        image_ML_class = document['properties']['image_ML_class'] # use this to assign to appropriate subfolder residential or nonresidential\n",
    "        response = client.get_document(\n",
    "            db=config[\"DB_NAME\"],\n",
    "            doc_id=document[\"_id\"],\n",
    "            attachments=True,\n",
    "        ).get_result()\n",
    "\n",
    "        for attachment_name, attachment_info in response['_attachments'].items():\n",
    "            attachment_data = base64.b64decode(attachment_info['data'])\n",
    "            attachment_object = {\n",
    "                \"id\": document[\"_id\"],\n",
    "                \"data\": cv2.imdecode(np.frombuffer(attachment_data, np.uint8), cv2.IMREAD_COLOR),\n",
    "            }\n",
    "\n",
    "            # Store cv2 image locally\n",
    "\n",
    "            if image_ML_type == 'train':\n",
    "                if image_ML_class == 'residential':\n",
    "                    save_path, name_appendix = train_res_path, \"_train_res_.png\"\n",
    "                elif image_ML_class == 'nonresidential':\n",
    "                    save_path, name_appendix = train_nonres_path, \"_train_nonres_.png\"\n",
    "\n",
    "            elif image_ML_type == 'test':\n",
    "                if image_ML_class == 'residential':\n",
    "                    save_path, name_appendix = test_res_path, \"_test_res_.png\"\n",
    "                elif image_ML_class == 'nonresidential':\n",
    "                    save_path, name_appendix = test_nonres_path, \"_test_nonres_.png\"\n",
    "\n",
    "            elif image_ML_type == 'validation':\n",
    "                if image_ML_class == 'residential':\n",
    "                    save_path, name_appendix = validation_res_path, \"_validation_res_.png\"\n",
    "                elif image_ML_class == 'nonresidential':\n",
    "                    save_path, name_appendix = validation_nonres_path, \"_validation_nonres_.png\"\n",
    "\n",
    "            file_name = f\"/osm_id_{response['properties']['osm_id']}_lon_{response['_id'].split(':')[0]}_lat_{response['_id'].split(':')[1]}\"\n",
    "            cv2.imwrite(save_path + file_name + name_appendix, attachment_object['data'])\n",
    "            time.sleep(0.005)\n",
    "    except Exception as e:\n",
    "        print(f\"Document id {document['_id']} Exception occured: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6155 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# define data generator with test data\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(124, 124),\n",
    "    batch_size=len(os.listdir(test_res_path)) + len(os.listdir(test_nonres_path)),\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res images amount: 5280, Nonres images amount: 875\n"
     ]
    }
   ],
   "source": [
    "# len(os.listdir(test_res_path)) + len(os.listdir(test_nonres_path))\n",
    "print(f'Res images amount: {len(os.listdir(test_res_path))}, Nonres images amount: {len(os.listdir(test_nonres_path))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CustomArchitecture_CFG010_DenseNet121_dt09_06_2023_01_05_44.h5',\n",
       " 'Baseline_CFG009_ResNet50_dt09_05_2023_01_06_23.h5']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download models\n",
    "models_dir = os.path.join(base_path, 'models')\n",
    "os.makedirs(models_dir, exist_ok = True)\n",
    "\n",
    "model_names = ['Baseline_CFG009_ResNet50_dt09_05_2023_01_06_23.h5', 'CustomArchitecture_CFG010_DenseNet121_dt09_06_2023_01_05_44.h5']\n",
    "model_paths = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    \n",
    "    model_path = os.path.join(models_dir, model_name)\n",
    "    model_paths.append(model_path)\n",
    "    \n",
    "    fobj = data_conn.Object(config[\"ML_MODELS_BUCKET\"], model_name).get()['Body'].read()\n",
    "    fobj = io.BytesIO(fobj)\n",
    "\n",
    "    with open(model_path, 'wb') as outfile:\n",
    "        outfile.write(fobj.getbuffer())\n",
    "\n",
    "        \n",
    "os.listdir(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(idx, model_name):\n",
    "    '''\n",
    "    Evaluate model function performs scoring of model based on test part of the dataset\n",
    "    Result of function execution is a confucion matrix\n",
    "    '''\n",
    "    \n",
    "    print(f\"Evaluating model {model_name}\")\n",
    "    model = keras.models.load_model(os.path.join(models_dir, model_name))\n",
    "    print()\n",
    "    images, true_labels = test_generator.next()\n",
    "    true_labels = [int(j) for j in true_labels]\n",
    "\n",
    "    predictions = [1 if i[0] > 0.5 else 0 for i in model.predict(images)]\n",
    "    confusion_matrix = SKM.confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    metrics = {\n",
    "        'Accuracy': SKM.accuracy_score(true_labels, predictions),\n",
    "        'F1_Score': SKM.f1_score(true_labels, predictions),\n",
    "        'Precision': SKM.precision_score(true_labels, predictions),\n",
    "        'Recall': SKM.recall_score(true_labels, predictions),\n",
    "        }\n",
    "    \n",
    "    res_amount = len(os.listdir(test_res_path))\n",
    "    nonres_amount = len(os.listdir(test_nonres_path))\n",
    "    counts = [nonres_amount, nonres_amount, res_amount, res_amount]\n",
    "    group_names = ['Correctly predicted Nonres','Incorrectly predicted Res','Incorrectly predicted Nonres','Correctly predicted Res']\n",
    "#     group_counts = [\"{0:0.0f}\".format(value) for value, count in zip(confusion_matrix.flatten(), counts)]\n",
    "    group_percentages_and_counts = [f\"{round(100*value/count, 2)} %\\n {value} of {count}\" for value, count in zip(confusion_matrix.flatten(), counts)]\n",
    "    print(confusion_matrix.flatten())\n",
    "    labels = [f\"{v1}\\n{v3}\" for v1, v3 in zip(group_names,group_percentages_and_counts)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "\n",
    "    x_axis_labels = ['Predicted nonres', 'Predicted res']\n",
    "    y_axis_labels = ['Actual nonres', 'Actual res']\n",
    "    print(confusion_matrix)\n",
    "    heatmap = sns.heatmap(\n",
    "        SKM.confusion_matrix(true_labels, predictions, normalize='true'),\n",
    "        ax=axes[idx],\n",
    "        xticklabels=x_axis_labels,\n",
    "        yticklabels=y_axis_labels,\n",
    "        annot=labels, \n",
    "        fmt='')\n",
    "    \n",
    "    heatmap.set(\n",
    "        title='Confusion matrix',\n",
    "        xlabel='Predicted label',\n",
    "        ylabel='Actual label'\n",
    "    )\n",
    "\n",
    "    heatmap.text(0, 2.45, f\"Model: {model_name}\", fontsize = 11)\n",
    "#     scores = model.evaluate(test_generator, verbose=0)\n",
    "    for idx, (metric, value) in enumerate(metrics.items()):\n",
    "        heatmap.text(0, 0.11*idx + 2.6, f\"{metric}: {round(value, 4)}\", fontsize = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run evaluation over selected models\n",
    "fig, axes = plt.subplots(1, len(model_paths), figsize=(25, 7))\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    evaluate_model(idx, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
