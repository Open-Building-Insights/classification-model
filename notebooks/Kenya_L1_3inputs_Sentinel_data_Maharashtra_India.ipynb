{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kenya_L1_3inputs_Sentinel_data_Maharashtra_India\n",
    "### This notebook trains the model to classify buildings based on their roof image, SMOD classification and footprint area\n",
    "### The input is provided as a Parquet file in a COS bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial configuration\n",
    "#### To start working with this particular notebook, you need to provide necessary credential and settings\n",
    "#### Below is an template of configuration, which is necessary prepare aside of this notebook and copy & paste all content in triple quotes to the next cell's input field\n",
    "    \"\"\"\n",
    "    {\n",
    "    \"ML_MODELS_BUCKET\": \"l2-ml-saved-models\",\n",
    "    \"ML_MODELS_BUCKET_CRN\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/a7177a43510743b19505df1df3241e00:4f46d3ea-d81a-4b98-acdc-5ae9f9315699::\",\n",
    "    \"UTILS_BUCKET\": \"notebook-utils-bucket\",\n",
    "    \"COS_ENDPOINT_URL\": \"https://s3.eu-de.cloud-object-storage.appdomain.cloud\",\n",
    "    \"COS_AUTH_ENDPOINT_URL\": \"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    \"COS_APIKEY\": \"xxx\"\n",
    "    }\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read notebook configuration\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "config_str = getpass.getpass('Enter your prepared config: ')\n",
    "config = json.loads(config_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# import keras_tuner as kt\n",
    "import fnmatch\n",
    "import cv2\n",
    "import gc\n",
    "# from ttictoc import tic, toc\n",
    "import csv\n",
    "import random\n",
    "import datetime\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import layers, models\n",
    "from keras import backend as B\n",
    "from tensorflow.keras.applications import DenseNet121, EfficientNetV2B3\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LambdaCallback, Callback\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad\n",
    "\n",
    "from keras.layers import Input, Concatenate, Conv2D, Dense, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "import sklearn.preprocessing as SKP\n",
    "import numpy as np\n",
    "import shutil\n",
    "import io\n",
    "import ibm_boto3\n",
    "from botocore.client import Config\n",
    "import tarfile\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import scipy as SCP\n",
    "import psutil\n",
    "import os\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "# from ibmcloudant.cloudant_v1 import CloudantV1\n",
    "# from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "import cv2\n",
    "\n",
    "# from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = 'DenseNet121_L1_1I_2N'\n",
    "configuration = 'CFG001_India_Maharashtra'\n",
    "\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "                              ibm_api_key_id=config[\"COS_APIKEY\"],\n",
    "                              ibm_service_instance_id=config[\"ML_MODELS_BUCKET_CRN\"],\n",
    "                              ibm_auth_endpoint=config[\"COS_AUTH_ENDPOINT_URL\"],\n",
    "                              config=Config(signature_version='oauth'),\n",
    "                              endpoint_url=config[\"COS_ENDPOINT_URL\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1ml_datasets_bucket = cos_client.Bucket('l1ml-datasets')\n",
    "\n",
    "dataset_name = 'ML_dataset_Maharashtra_ver1_SMOD_heights_images.parquet'\n",
    "l1ml_datasets_bucket.download_file(dataset_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ML_df = pd.read_parquet(dataset_name)\n",
    "\n",
    "\n",
    "ML_df = ML_df[(ML_df.use_for_training == 'Yes') & (ML_df.vida_confidence >= 0.9) & ~ML_df.image_source_bytes.isnull()]\n",
    "\n",
    "ML_df[['image_ML_type', 'L1_class']].groupby(['image_ML_type', 'L1_class']).agg({'L1_class': ['count']})\n",
    "\n",
    "ML_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gML = ML_df[['image_ML_type', 'L1_class']].reset_index().groupby(['image_ML_type', 'L1_class'])[\"L1_class\"].count().reset_index(name=\"count\")\n",
    "gML = pd.DataFrame(gML)\n",
    "gML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_df[ML_df.L1_class == 'residential'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(ML_df[ML_df.L1_class == 'residential'].building_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(ML_df[ML_df.L1_class == 'nonresidential'].building_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "data_path = os.path.join(base_path, 'Sentinel_set/')\n",
    "\n",
    "models_dir = os.path.join(base_path, 'model_files')\n",
    "checkpoints_and_metadata = os.path.join(base_path, 'model_checkpoints_and_metadata')\n",
    "\n",
    "# delete models dir if exists\n",
    "try:\n",
    "    shutil.rmtree(models_dir)\n",
    "    shutil.rmtree(checkpoints_and_metadata)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# recreste models dir\n",
    "os.makedirs(models_dir, exist_ok = True)\n",
    "os.makedirs(checkpoints_and_metadata, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To ensure that necessary folders were created\n"
     ]
    }
   ],
   "source": [
    "print('To ensure that necessary folders were created')\n",
    "# os.listdir('/home/wsuser/work/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_class_number = {\n",
    "    'nonresidential': 0,\n",
    "    'residential': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_area = 20_000\n",
    "normalize_height = 20\n",
    "normalize_smod = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_tree = {\n",
    "    'train': ['nonresidential', 'residential'],\n",
    "    'test': ['nonresidential', 'residential'],\n",
    "    'validation': ['nonresidential', 'residential'],\n",
    "}\n",
    "\n",
    "train_images, train_numeric, train_labels = [], [], []\n",
    "validation_images, validation_numeric, validation_labels = [], [], []\n",
    "test_images, test_numeric, test_labels = [], [], []\n",
    "\n",
    "\n",
    "def create_learning_sample(row):\n",
    "    \n",
    "    image_data = base64.b64decode(row.image_source_bytes)\n",
    "    img = Image.open(io.BytesIO(image_data))\n",
    "    reshaped_img = np.array(img.resize((124, 124), Image.Resampling.NEAREST))\n",
    "    output = SKP.label_binarize([get_class_number[row.L1_class]], classes=np.arange(2))[0]\n",
    "    \n",
    "    return reshaped_img, output\n",
    "\n",
    "# limit = 10_000\n",
    "\n",
    "for type_folder, class_folders in folders_tree.items():\n",
    "    \n",
    "#     c_counter = {\n",
    "#         'nonresidential': 0,\n",
    "#         'residential': 0,\n",
    "#     }\n",
    "    \n",
    "    for classfolder in class_folders:\n",
    "        \n",
    "        folder_path = os.path.join(data_path, type_folder, classfolder)\n",
    "        \n",
    "        class_images = ML_df[(ML_df.image_ML_type == type_folder) & (ML_df.L1_class == classfolder)]\n",
    "        \n",
    "        for img_idx, row in enumerate(class_images.itertuples()):\n",
    "              \n",
    "#             if c_counter[row.L1_class] > limit: break\n",
    "            \n",
    "            if type_folder == \"train\":\n",
    "                \n",
    "                reshaped_img, output = create_learning_sample(row)\n",
    "                \n",
    "                train_images.append(reshaped_img)\n",
    "                train_numeric.append([row.area_in_meters / normalize_area, row.SMOD_id/normalize_smod])\n",
    "                train_labels.append(output)\n",
    "                \n",
    "            elif type_folder == \"validation\":\n",
    "                \n",
    "                reshaped_img, output = create_learning_sample(row)\n",
    "\n",
    "                validation_images.append(reshaped_img)\n",
    "                validation_numeric.append([row.area_in_meters / normalize_area, row.SMOD_id/normalize_smod])\n",
    "                validation_labels.append(output)\n",
    "                \n",
    "            elif type_folder == \"test\":\n",
    "                \n",
    "                reshaped_img, output = create_learning_sample(row)\n",
    "                \n",
    "                test_images.append(reshaped_img)\n",
    "                test_numeric.append([row.area_in_meters / normalize_area, row.SMOD_id/normalize_smod])\n",
    "                test_labels.append(output)\n",
    "                \n",
    "#             c_counter[row.L1_class] += 1\n",
    "\n",
    "len_train_images = len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "batch_size=32\n",
    "# Define the image transformations here\n",
    "gen = ImageDataGenerator(\n",
    "                            horizontal_flip = True, \n",
    "                            vertical_flip = True, \n",
    "                            rotation_range = 20,\n",
    "                        )\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_two_inputs(X1, X2, y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=123)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=123)\n",
    "    \n",
    "    while True:\n",
    "        X1i = genX1.next()\n",
    "        X2i = genX2.next()\n",
    "        \n",
    "        #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "#         np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "\n",
    "        yield [X1i[0], X2i[1]], X1i[1]\n",
    "\n",
    "# Finally create generator\n",
    "\n",
    "train_images, train_numeric, train_labels = np.array(train_images), np.array(train_numeric), np.array(train_labels)\n",
    "validation_images, validation_numeric, validation_labels = np.array(validation_images), np.array(validation_numeric), np.array(validation_labels)\n",
    "\n",
    "train_generator = gen_flow_for_two_inputs(train_images, train_numeric, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collector: collected 118 objects.\n"
     ]
    }
   ],
   "source": [
    "del train_images\n",
    "del train_numeric\n",
    "del train_labels\n",
    "\n",
    "collected = gc.collect()\n",
    "# Prints Garbage collector \n",
    "# as 0 object\n",
    "print(\"Garbage collector: collected %d objects.\" % collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAP2D_layer created\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image_input (InputLayer)       [(None, 124, 124, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " numeric_input (InputLayer)     [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " densenet121 (Functional)       (None, 3, 3, 1024)   7037504     ['image_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4)            12          ['numeric_input[0][0]']          \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1024)        0           ['densenet121[0][0]']            \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1028)         0           ['dense[0][0]',                  \n",
      "                                                                  'global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1028)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 200)          205800      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 100)          20100       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 100)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 50)           5050        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            51          ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,268,517\n",
      "Trainable params: 7,184,869\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define two input layers\n",
    "image_input = Input((124, 124, 3), name=\"image_input\")\n",
    "numeric_input = Input((2,), name=\"numeric_input\")\n",
    "\n",
    "# Convolution + Flatten for the image\n",
    "base_cnn_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(124, 124, 3))\n",
    "base_cnn_model.trainable = True\n",
    "\n",
    "base_cnn_model_input = base_cnn_model(image_input)\n",
    "\n",
    "numeric_input_layer = Dense(4, activation='tanh', use_bias=True)(numeric_input)\n",
    "\n",
    "GAP2D_layer = GlobalAveragePooling2D()(base_cnn_model_input)\n",
    "\n",
    "print('GAP2D_layer created')\n",
    "# Concatenate the convolutional features and the vector input\n",
    "concat_layer = Concatenate()([numeric_input_layer, GAP2D_layer])\n",
    "\n",
    "dropout_layer = Dropout(0.4)(concat_layer)\n",
    "dense_layer = Dense(200, activation='elu', use_bias=True)(dropout_layer)\n",
    "dropout_layer = Dropout(0.4)(dense_layer)\n",
    "dense_layer = Dense(100, activation='elu', use_bias=True)(dropout_layer)\n",
    "dropout_layer = Dropout(0.4)(dense_layer)\n",
    "dense_layer = Dense(50, activation='elu', use_bias=True)(dropout_layer)\n",
    "output = Dense(1, activation='sigmoid')(dense_layer)\n",
    "\n",
    "# define a model with a list of two inputs\n",
    "model = Model(inputs=[image_input, numeric_input], outputs=output)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sklearn.metrics as SKM\n",
    "import seaborn as sns\n",
    "\n",
    "file_writer_cm = tf.summary.create_file_writer(checkpoints_and_metadata)\n",
    "test_inputs = [np.array(test_images), np.array(test_numeric)]\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "nonres_count = gML[(gML.image_ML_type == 'test') & (gML.L1_class == 'nonresidential')]['count'].iloc[0]\n",
    "res_count = gML[(gML.image_ML_type == 'test') & (gML.L1_class == 'residential')]['count'].iloc[0]\n",
    "\n",
    "def plot_to_image(figure):\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close(figure)\n",
    "    buf.seek(0)\n",
    "\n",
    "    digit = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    digit = tf.expand_dims(digit, 0)\n",
    "\n",
    "    return digit\n",
    "\n",
    "def plot_confusion_matrix():\n",
    "    \n",
    "    print(f\"Evaluating model \")\n",
    "     \n",
    "    predictions = model.predict(test_inputs, verbose=1, batch_size=len(test_labels))\n",
    "\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "    cf_mtx = SKM.confusion_matrix(test_labels, y_pred)\n",
    "    # group_counts = [\"{0:0.0f}\".format(value) for value in cf_mtx.flatten()]\n",
    "    # group_percentages = [\"{0:.2%}\".format(value) for value in cf_mtx.flatten()/np.sum(cf_mtx)]\n",
    "\n",
    "    counts = [\n",
    "        nonres_count, nonres_count,\n",
    "        res_count, res_count,  \n",
    "        ]\n",
    "    \n",
    "    group_names = [\n",
    "        'Correctly predicted nonres', 'Incorrectly predicted nonres',\n",
    "        'Incorrectly predicted res', 'Correctly predicted res',\n",
    "        ]\n",
    "    #     group_counts = [\"{0:0.0f}\".format(value) for value, count in zip(confusion_matrix.flatten(), counts)]\n",
    "    group_percentages_and_counts = [f\"{round(100*value/count, 2)} %\\n {value} of {count}\" for value, count in zip(cf_mtx.flatten(), counts)]\n",
    "\n",
    "    box_labels = [f\"{v1}\\n {v2}\" for v1, v2 in zip(group_names, group_percentages_and_counts)]\n",
    "    box_labels = np.asarray(box_labels).reshape(2, 2)\n",
    "\n",
    "    figure, axes = plt.subplots(1, 1, figsize=(11, 9))\n",
    "\n",
    "    heatmap = sns.heatmap(cf_mtx, xticklabels=get_class_number.keys(), yticklabels=get_class_number.keys(), cmap=\"PiYG\", fmt=\"\", ax=axes, annot=box_labels)\n",
    "\n",
    "    heatmap.set(\n",
    "        title='Confusion matrix',\n",
    "        xlabel='Predicted label',\n",
    "        ylabel='Actual label'\n",
    "    )\n",
    "\n",
    "    return figure\n",
    "\n",
    "def log_confusion_matrix(epoch, logs):\n",
    " \n",
    "    figure = plot_confusion_matrix()\n",
    "    cm_image = plot_to_image(figure)\n",
    "\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\n",
    "        \n",
    "        \n",
    "class MemoryUsageCallback(Callback):\n",
    "    def on_epoch_begin(self,epoch,logs=None):\n",
    "        print('Memory usage on epoch begin: {} GB'.format(round(psutil.Process(os.getpid()).memory_info().rss / (1024**3), 3)))\n",
    "\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        print('Memory usage on epoch end:   {} GB'.format(round(psutil.Process(os.getpid()).memory_info().rss / (1024**3), 3)))\n",
    "        \n",
    "class ClearMemory(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):        \n",
    "        collected = gc.collect()\n",
    "        print(f\"Epoch {epoch}: garbage collector collected {collected} objects.\")\n",
    "#         k.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an appropriate optimizer\n",
    "# SGD, RMSprop, Adam, Adadelta, adagrad\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "B.clear_session()\n",
    "# Compile the model for multi-class classification\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "#     run_eagerly=True,\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),  # Use CategoricalCrossentropy for multi-class classification\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.TruePositives(name='tp'),\n",
    "        tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "        tf.keras.metrics.FalsePositives(name='fp'),\n",
    "        tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "        tf.keras.metrics.AUC(name='prc', curve='PR')  # Precision-Recall Curve\n",
    "    ])\n",
    "\n",
    "# Reducing learning rate for better model fit performance\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=1, verbose=1, min_delta=0.0005, min_lr=1e-13)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=checkpoints_and_metadata, histogram_freq=1)\n",
    "# early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, min_delta=0.005)\n",
    "log_confusion_matrix = LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
    "model_checkpoint = ModelCheckpoint(os.path.join(checkpoints_and_metadata, \"Epoch_{epoch:02d}-val_accuracy_{val_accuracy:.2f}_checkpoint.h5\"), monitor=\"val_accuracy\", verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage on epoch begin: 21.278 GB\n",
      "Epoch 1/20\n",
      " 337/1437 [======>.......................] - ETA: 30:57 - loss: 0.6311 - accuracy: 0.6904 - tp: 5222.0000 - tn: 2223.0000 - fp: 2056.0000 - fn: 1283.0000 - precision: 0.7175 - recall: 0.8028 - auc: 0.7229 - prc: 0.7706"
     ]
    }
   ],
   "source": [
    "# Fitting the model on the training data with further evaluation on the validation data during the training process\n",
    "hist = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=int(len_train_images / batch_size),\n",
    "    callbacks=[reduce_learning_rate, tensorboard_callback, model_checkpoint, MemoryUsageCallback(), ClearMemory()],\n",
    "    validation_data=([validation_images, validation_numeric], validation_labels),\n",
    "    shuffle=False,\n",
    "#     use_multiprocessing=True,\n",
    "#     workers=8,\n",
    "    verbose=1)\n",
    "\n",
    "# save model learning history\n",
    "with open(f\"{checkpoints_and_metadata}/model_history.json\", \"w\") as outfile:\n",
    "    json.dump(str(hist.history), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(acc: list, \n",
    "                        val_acc: list, \n",
    "                        loss: list, \n",
    "                        val_loss: list, \n",
    "                        precision: list, \n",
    "                        val_precision: list, \n",
    "                        recall: list, \n",
    "                        val_recall: list, \n",
    "                        f1: list, \n",
    "                        val_f1: list, \n",
    "                        prc: list, \n",
    "                        val_prc: list):\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0,1.0])\n",
    "    plt.title('Training and Validation Loss')\n",
    "    \n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(f1, label='F1-Score')\n",
    "    plt.plot(val_f1, label='Validation F1-Score')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title('Training and Validation F1-Score')\n",
    "    \n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.plot(prc, label='AUC-Prec-Recall')\n",
    "    plt.plot(val_prc, label='Validation AUC-Prec-Recall')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('AUC-Prec-Recall')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title('Training and Validation AUC-Prec-Recall')\n",
    "    plt.xlabel('epoch')\n",
    "            \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train_scores = [2*p*r/(p+r) if p !=0 or r != 0 else 0 for p, r in zip(hist.history['precision'], hist.history['recall'])]\n",
    "f1_val_scores = [2*p*r/(p+r) if p !=0 or r != 0 else 0 for p, r in zip(hist.history['val_precision'], hist.history['val_recall'])]\n",
    "\n",
    "plt = plot_learning_curves(\n",
    "                        hist.history[\"accuracy\"], \n",
    "                        hist.history[\"val_accuracy\"], \n",
    "                        hist.history[\"loss\"], \n",
    "                        hist.history[\"val_loss\"], \n",
    "                        hist.history[\"precision\"], \n",
    "                        hist.history[\"val_precision\"], \n",
    "                        hist.history[\"recall\"], \n",
    "                        hist.history[\"val_recall\"], \n",
    "                        f1_train_scores, \n",
    "                        f1_val_scores,\n",
    "                        hist.history[\"prc\"], \n",
    "                        hist.history[\"val_prc\"]\n",
    "                        )\n",
    "\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Garbage collector: collected: \", gc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = os.listdir(checkpoints_and_metadata)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(checkpoints_and_metadata):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file), \n",
    "                       os.path.relpath(os.path.join(root, file), \n",
    "                                       os.path.join(path, '..')))\n",
    "\n",
    "zip_filename = f\"HybridArchitecture_{configuration}_{architecture}_dt{datetime.datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}_checkpoints_and_metadata.zip\"\n",
    "            \n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipdir('/', zipf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"HybridArchitecture_{configuration}_{architecture}_dt{datetime.datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.h5\"\n",
    "model_path = os.path.join(models_dir, model_name)\n",
    "\n",
    "# tf.saved_model.save(model, model_path)\n",
    "model.save(filepath=model_path, overwrite=True)\n",
    "h5_model_Body=open(model_path, 'rb').read()\n",
    "cos_client.Object(config[\"ML_MODELS_BUCKET\"], model_name).upload_fileobj(io.BytesIO(h5_model_Body))\n",
    "print(f'Model uploaded to the Object Cloud Storage ml-saved-models bucket, model name: {model_name}')\n",
    "\n",
    "\n",
    "zip_Body=open(zip_filename, 'rb').read()\n",
    "cos_client.Object(config[\"ML_MODELS_BUCKET\"], zip_filename).upload_fileobj(io.BytesIO(zip_Body))\n",
    "\n",
    "print(f'Checkpoints and metadata were uploaded to the Object Cloud Storage ml-saved-models bucket, model name: {zip_filename}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
